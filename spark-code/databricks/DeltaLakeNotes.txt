Delta Lake:
--------------
Merge into
1. delta table, view or path (delta.'/tmp/source')
2. when matched (and condition1 then delete or update * or update set - so on) 
3. when not matched (and condition1 then insert* or insert cols - so on)
4. when matched by source (and condition1 then delete or update set - so on) -- hint: here condition1 helps to avoid full target write

Table:
1. Create : 
	Dataframe.write.saveAsTable('tablename')
	Create table if not exists <dml>
	create or replace table <dml>
	create table <t2> like <t1>
	DeltaTable.createIfNotExist(spark).tableName('t1').addColumn('c1','INT').execute()
	DeltaTable.createIfNotExist(spark).addColumn('c1','INT').location('/tmp/source').execute()
	DeltaTable.createOrReplace(spark).tableName('t1').addColumn('c1','INT').execute()
	DeltaTable.createOrReplace(spark).addColumn('c1','INT').location('/tmp/source').execute()
2. Insert into
	Insert into table -- append data
	Insert overwrite into table -- replace data
3. Update 
	Update table set
	Update delta.'/tmp/source' set
4. Delete
	Delete from table where condition1
	Delete from delta.'/tmp/source' where condition1
5. Describe
	Describe history table - version , operation, changes
6. Time travel
	select * from table version as of <version>
	select * from table timestamp as of <timestamp> -- can be a date or timestamp
7. Optimize 
	optimze tablename -- small file into large ones -- does have an impact on history? - to be checked in handson
	Optimize tablename ZORDER BY <column> - group same value of column into same files, does have an impact on history? - to be checked in handson
8. vacuum
	vacuum tablename -- remove unused snapshots/files
	
Selective overwrites
1. replaceWhere 
-- df.write.mode("overwrite").option("replaceWhere",condition1).save(filepath) 
-- insert into table t1 replace where condition1 select * from replace_data
-- replace the matching data in filepath with df
-- fails if the df/replace_data contains additionl data outside of the condition1
-- additional data outside of the condition1 can be insert with the checkConstraint has been disabled with config
-- also, older version only condition1 with partition column works, it can be enabled by disabling the datacolumns flag in th config
2. Dynamic Overwrties
-- can be enabled via partitionOverwriteMode=dynamic, default=static
--df.write.mode("overwrite").option("partitionOverwriteMode", "dynamic").save(filepath)
--insert overwrite table t1 select * from df
--replaces the values of matching partition from df
-- df.write option has precedence than session config
-- df.write both options fails
-- df.write option overwriteschema is not allowed
-- how about new data , guess will be inserted -- to be checked

History log and data retention, time travel, restore, transaction log checkpoints
-- Describe history table or 'path' or delta.'path'
-- Describe history table limit 1 -- only last one
-- stored in storage alongside with data fuels in directory _delta_log
-- logretentionduration default (table/file level) is 30 days
-- contains version, timestamp, user , actions, metrics, etc 
-- vacumm deletes old unused data files, 
-- vacuum - if runs daily with default config deletedfileretentionduration = 7 days (default)
-- Time travel 
	- select * from table as of version or date or timestamp or timestamp_expression (subqueries not allowed)
	- also, select * from table@[version, yyyyMMddHHmmssSSS]
-- transaction log checkpoints - optimized version json log files into parquet checkpoints file
-- restore 
	-- restore table t1 to version as of <version>
	-- restore table t1 to timestamp as of <timestamp>
	-- restore not possible if old files where deleted manually or by vacuum
	-- it is also change data operation , adds new version in transaction log and trigger downstream app e.g. streaming , so may cause data duplicates

Vacuum
-- vacumm deletes old unused data files, 
-- vacuum - if runs daily with default config deletedfileretentionduration = 7 days (default)
-- VACUUM eventsTable   -- vacuum files not required by versions older than the default retention period
-- VACUUM '/data/events' -- vacuum files in path-based table
-- VACUUM delta.`/data/events/`
-- VACUUM delta.`/data/events/` RETAIN 100 HOURS  -- vacuum files not required by versions more than 100 hours old
-- VACUUM eventsTable DRY RUN    -- do dry run to get the list of files to be deleted
-- still topics to cover

Liquid clustering
-- replaces partition and z order, improve query performance using cluster keys
-- used when filter by high cardinality columns, high skew data distribution, table grow quickly/ high tuning maintenance, concurrent writes, acces pattern changes
-- create table t1 (cols ) using delta cluster by (col);
-- create external table t1 cluster by (col) location 'filepath' as select * from t2;
-- create table t1 like t2 -- copies the configuration of cluster by of t2
-- converting choosing cluster keys -- partition . z order column , 
-- reduce conflicts b/w row level concurrent operations such as optimize, insert ,merge , update or delete.
-- writing to cluster table 
-- Create table t1 cluster by col as select * from t2
-- insert into , copy into from parquet,  spark.write.format('delta').mode('append')
-- optimize t1 , trigger clustering
-- read cluster table , select * from t1 where cluster_key_col = value
-- change cluster keys, alter table t1 cluster by (col1, col2) -- note: only subsequent write consider new cluster keys
-- remove cluster keys , alter table t1 cluster by none -- note : does not rewrite the data already clustered
-- describe table or detail t1 -- view cluster keys
-- cluster key work only with columns collected statistics , default only 32 cols (count each field if nested), max. 4 keys, structured streaming not supported

Data Skipping and Z order
-- dataSkippingNumIndexedCols helps to add mode cols collecting statistics, 
-- stats collect is expensive for expensive long string cols, to avoid either config dataSkippingNumIndexedCols or move cols position larger than the config.
-- Z order to move the data colocated for specified cols , is not idempotent
-- optimize table (where col1 condition1) zorder by col2; -- collect stats is important for zorder to work , otherwise its ineffective

Optimize
-- improve query performance by coalesce small files to large ones
-- is idempotent, re-run have same effect
-- Optimize t1, optimize delta.'filepath', subset only then : optimize table where condition1;

Change Data Feed
-- Table level changes such as (change types) Insert, Update, delete aare captured along with version and timestamp in addition to table data.
-- enabled using the configuration delta.enableChangeDataFeed = true, changes are captured only from the time enabled
-- Only following operations are effect - Merge, udpate , delete, insert only or partition delete do not have any effect.
-- Enable statement 
	- CREATE TABLE student (id INT, name STRING, age INT) TBLPROPERTIES (delta.enableChangeDataFeed = true)
	- ALTER TABLE myDeltaTable SET TBLPROPERTIES (delta.enableChangeDataFeed = true)
	- for all new tables : set spark.databricks.delta.properties.defaults.enableChangeDataFeed = true;
-- List table changes
	- SELECT * FROM table_changes('tableName', 0, 10) -- Version start and end
	- SELECT * FROM table_changes('tableName', '2021-04-21 05:45:46', '2021-05-21 12:00:00') -- timestamp start and end
	- SELECT * FROM table_changes('tableName', 0) -- version start only until latest
	- SELECT * FROM table_changes('dbName.`dotted.tableName`', '2021-04-21 06:45:46' , '2021-05-21 12:00:00') -- escape char for . when schema
	- SELECT * FROM table_changes_by_path('\path', '2021-04-21 05:45:46') -- when path specified
-- Changes are stored in the same location as data under directory _change_data
-- table changes have the same retention duration as the data e.g. deletedfileretentionduration, so VACUUM command delete changes along with data
-- Reading changes as streaming
	- with start version -- spark.readStream.format("delta").option("readChangeFeed", "true").option("startingVersion", 0).table("myDeltaTable")
	- with start timestamp -- spark.readStream.format("delta").option("readChangeFeed", "true").option("startingTimestamp", "2021-04-21 05:35:43").load("/pathToMyDeltaTable")
	- not providing version or timestamp so reads latest snapshot as insert - spark.readStream.format("delta").option("readChangeFeed", "true").table("myDeltaTable")
-- OutofRange Error 
	-- by default when user passes version or timestamp specified greater than last commit version or timestamp , an error is thrown
	-- to supress the error , enable set spark.databricks.delta.changeDataFeed.timestampOutOfRange.enabled = true; 
		-- when start version or timestamp specified greater than last commit version or timestamp, return empty result instead or error
		-- when end version or timestamp specified greater than last commit version or timestamp, return changes until last commit instead of error

Column Mapping
-- Support changes to metadata e.g. delete or rename columns without making changes to data files
-- Also allows to use chars in the column names which is not possible with Parquet such as spaces
-- following chars allowed as well : ,;{}()\n\t=
-- enabling column mapping of table
	-  ALTER TABLE <table-name> SET TBLPROPERTIES ('delta.minReaderVersion' = '2','delta.minWriterVersion' = '5','delta.columnMapping.mode' = 'name')
	- once enabled , can not be disabled . e.g 'delta.columnMapping.mode' = 'none' thrown an error
-- Rename a column - ALTER TABLE <table-name> RENAME COLUMN old_col_name TO new_col_name
-- Drop column(s) 
	- ALTER TABLE table_name DROP COLUMN col_name
	- ALTER TABLE table_name DROP COLUMNS (col_name_1, col_name_2, ...)
-- schema tracking for streaming to support column mapping , otherwise streams break with non-additive schemas
	- table (read) schemaTrackingLocation configured to checkPointLocation of Target (write) e.g. as below
	- checkpoint_path = "/path/to/checkpointLocation"
	- (spark.readStream.option("schemaTrackingLocation", checkpoint_path).table("delta_source_table")
		.writeStream.option("checkpointLocation", checkpoint_path).toTable("output_table"))
		
Table Constraints
-- supports two on delta lake such as Enforced constraints (ensure data quality and integrity) and Primary/Foreign keys (relationship b/w tables via columns)
-- Enforced constraints are not null, checkConstraint e.g.
	- CREATE TABLE people10m (id INT NOT NULL, firstName STRING NOT NULL, birthDate TIMESTAMP) USING DELTA; 
	- ALTER TABLE people10m ALTER COLUMN firstName DROP NOT NULL;
	- ALTER TABLE people10m ALTER COLUMN firstName SET NOT NULL;
	- for nested column , NOT NULL should satisfy the parent also NOT NULL, columns nested in arrays or map do not support NOT NULL constraints
	- ALTER TABLE people10m ADD CONSTRAINT dateWithinRange CHECK (birthDate > '1900-01-01');
	- ALTER TABLE people10m DROP CONSTRAINT dateWithinRange;
	- view the constraints by running - DESCRIBE DETAIL people10m or SHOW TBLPROPERTIES people10m
-- Primary/Foreign key constraints
	- Primary key and foreign key constraints require Unity Catalog and Delta Lake.
	- Primary and foreign keys are informational only and are not enforced. 
	- CREATE TABLE T(pk1 INTEGER NOT NULL, pk2 INTEGER NOT NULL, CONSTRAINT t_pk PRIMARY KEY(pk1, pk2));
	- CREATE TABLE S(pk INTEGER NOT NULL PRIMARY KEY, fk1 INTEGER, fk2 INTEGER,CONSTRAINT s_t_fk FOREIGN KEY(fk1, fk2) REFERENCES T);
	- not allowed in CTAS statement
	
View Table details
-- describes description, location , created at, last modefied, partition columns, no. of files , size in bytes, properties , minReaderVersion/minWriterVersion
-- DESCRIBE DETAIL '/data/events/'
-- DESCRIBE DETAIL eventsTable

User Defined Metadata
-- aka. custom metadata and custom tags of delta lake tables
-- fields in the Delta Lake transaction log to add custom tags to a table or messages for an individual commit.
-- done by using the DataFrameWriter option userMetadata or the SparkSession configuration spark.databricks.delta.commitInfo.userMetadata.
	- df.write.format("delta").mode("overwrite").option("userMetadata", "overwritten-for-fixing-incorrect-data").save("/tmp/delta/people10m")
	- SET spark.databricks.delta.commitInfo.userMetadata=overwritten-for-fixing-incorrect-data
	  INSERT OVERWRITE default.people10m SELECT * FROM morePeople
-- Store own metadata as a table property using TBLPROPERTIES in CREATE and ALTER.
	- ALTER TABLE default.people10m SET TBLPROPERTIES ('department' = 'accounting', 'delta.appendOnly' = 'true');
	- SHOW TBLPROPERTIES default.people10m; -- Show all the properties of table.
    - SHOW TBLPROPERTIES default.people10m ('department'); -- Show just the 'department' table property.


Generated Columns
--

Idempotent Writes
--

Deletion Vectors
--

Schema Validation









